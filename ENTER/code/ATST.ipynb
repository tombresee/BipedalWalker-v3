{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd05a10",
   "metadata": {},
   "source": [
    "* https://github.com/openai/gym/wiki/BipedalWalker-v2\n",
    "* https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f738fe",
   "metadata": {},
   "source": [
    "* winner almost: https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb964743",
   "metadata": {},
   "source": [
    "* review: https://github.com/createamind/DRL/blob/master/spinup/algos/sac1/sac1_BipedalWalker-v2_200ep.py\n",
    "* https://github.com/createamind/DRL/tree/master/spinup/envs/BipedalWalkerHardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff357d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q_1  = 100\n",
    "\n",
    "\n",
    "Q_theta_trained_sub_prime_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdac9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega: Ω\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Omega: \\u03A9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb679d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94594b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1016750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb83b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a58616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Θ = 100\n",
    "\n",
    "print(Θ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "γ * min(Α + θ + Γ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc5233b",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b374584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing.............................. solving bipedal walker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa6a43",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f003c",
   "metadata": {},
   "source": [
    "Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. \n",
    "\n",
    "Applying motor torque costs a small amount of points, more optimal agent will get better score.\n",
    "\n",
    "State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45637503",
   "metadata": {},
   "source": [
    "https://github.com/ZhiqingXiao/OpenAIGymSolution/blob/master/BipedalWalker-v3/bipedalwalker_v3_close_form.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d4f77",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1af4f8",
   "metadata": {},
   "source": [
    "Run it on a GPU... its much faster..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05186b",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf66c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took about 13 mins on cpu...\n",
    "# Ep. 1, Ep.Timesteps 267, Episode Reward: -63.57\n",
    "# Ep. 2, Ep.Timesteps 132, Episode Reward: -84.10\n",
    "# Ep. 3, Ep.Timesteps 1534, Episode Reward: 256.24\n",
    "# Ep. 4, Ep.Timesteps 1208, Episode Reward: 167.17\n",
    "# Ep. 5, Ep.Timesteps 1270, Episode Reward: 277.94\n",
    "# Ep. 6, Ep.Timesteps 1211, Episode Reward: 279.98\n",
    "# Ep. 7, Ep.Timesteps 1208, Episode Reward: 282.23\n",
    "# Ep. 8, Ep.Timesteps 86, Episode Reward: -125.83\n",
    "# Ep. 9, Ep.Timesteps 85, Episode Reward: -125.68\n",
    "# Ep. 10, Ep.Timesteps 64, Episode Reward: -117.39\n",
    "# Ep. 11, Ep.Timesteps 87, Episode Reward: -122.46\n",
    "# Ep. 12, Ep.Timesteps 75, Episode Reward: -120.37\n",
    "# Ep. 13, Ep.Timesteps 1374, Episode Reward: 276.25\n",
    "# Ep. 14, Ep.Timesteps 1269, Episode Reward: 282.50\n",
    "# Ep. 15, Ep.Timesteps 1255, Episode Reward: 279.71\n",
    "# Ep. 16, Ep.Timesteps 1190, Episode Reward: 282.08\n",
    "# Ep. 17, Ep.Timesteps 1193, Episode Reward: 281.95\n",
    "# Ep. 18, Ep.Timesteps 633, Episode Reward: 19.25\n",
    "# Ep. 19, Ep.Timesteps 1242, Episode Reward: 278.11\n",
    "# Ep. 20, Ep.Timesteps 1600, Episode Reward: -104.99\n",
    "# Ep. 21, Ep.Timesteps 1128, Episode Reward: 287.65\n",
    "# Ep. 22, Ep.Timesteps 1128, Episode Reward: 288.05\n",
    "# Ep. 23, Ep.Timesteps 1207, Episode Reward: 283.67\n",
    "# Ep. 24, Ep.Timesteps 1255, Episode Reward: 279.80\n",
    "# Ep. 25, Ep.Timesteps 1104, Episode Reward: 290.29\n",
    "# Ep. 26, Ep.Timesteps 1185, Episode Reward: 286.32\n",
    "# Ep. 27, Ep.Timesteps 1128, Episode Reward: 288.53\n",
    "# Ep. 28, Ep.Timesteps 928, Episode Reward: 64.39\n",
    "# Ep. 29, Ep.Timesteps 1163, Episode Reward: 288.67\n",
    "# Ep. 30, Ep.Timesteps 1600, Episode Reward: -90.03\n",
    "# Ep. 31, Ep.Timesteps 1153, Episode Reward: 289.69\n",
    "# Ep. 32, Ep.Timesteps 1150, Episode Reward: 289.55\n",
    "# Ep. 33, Ep.Timesteps 1268, Episode Reward: 284.66\n",
    "# Ep. 34, Ep.Timesteps 1218, Episode Reward: 285.93\n",
    "# Ep. 35, Ep.Timesteps 1224, Episode Reward: 286.26\n",
    "# Ep. 36, Ep.Timesteps 1248, Episode Reward: 285.02\n",
    "# Ep. 37, Ep.Timesteps 1234, Episode Reward: 283.74\n",
    "# Ep. 38, Ep.Timesteps 1229, Episode Reward: 284.27\n",
    "# Ep. 39, Ep.Timesteps 1228, Episode Reward: 283.60\n",
    "# Ep. 40, Ep.Timesteps 1185, Episode Reward: 287.51\n",
    "# Ep. 41, Ep.Timesteps 1185, Episode Reward: 288.11\n",
    "# Ep. 42, Ep.Timesteps 1183, Episode Reward: 287.22\n",
    "# Ep. 43, Ep.Timesteps 1110, Episode Reward: 292.43\n",
    "# Ep. 44, Ep.Timesteps 1199, Episode Reward: 286.46\n",
    "# Ep. 45, Ep.Timesteps 1200, Episode Reward: 286.88\n",
    "# Ep. 46, Ep.Timesteps 1201, Episode Reward: 286.33\n",
    "# Ep. 47, Ep.Timesteps 1157, Episode Reward: 288.89\n",
    "# Ep. 48, Ep.Timesteps 1153, Episode Reward: 289.29\n",
    "# Ep. 49, Ep.Timesteps 1137, Episode Reward: 289.58\n",
    "# Ep. 50, Ep.Timesteps 1138, Episode Reward: 290.90, Moving Average Reward: 202.25\n",
    "# Ep. 51, Ep.Timesteps 1150, Episode Reward: 292.60, Moving Average Reward: 209.38\n",
    "# Ep. 52, Ep.Timesteps 1102, Episode Reward: 293.62, Moving Average Reward: 216.93\n",
    "# Ep. 53, Ep.Timesteps 1085, Episode Reward: 294.70, Moving Average Reward: 217.70\n",
    "# Ep. 54, Ep.Timesteps 1134, Episode Reward: 291.71, Moving Average Reward: 220.19\n",
    "# Ep. 55, Ep.Timesteps 1063, Episode Reward: 296.84, Moving Average Reward: 220.57\n",
    "# Ep. 56, Ep.Timesteps 1100, Episode Reward: 295.54, Moving Average Reward: 220.88\n",
    "# Ep. 57, Ep.Timesteps 1094, Episode Reward: 295.55, Moving Average Reward: 221.15\n",
    "# Ep. 58, Ep.Timesteps 1149, Episode Reward: 292.50, Moving Average Reward: 229.51\n",
    "# Ep. 59, Ep.Timesteps 1115, Episode Reward: 293.32, Moving Average Reward: 237.89\n",
    "# Ep. 60, Ep.Timesteps 1140, Episode Reward: 293.68, Moving Average Reward: 246.12\n",
    "# Ep. 61, Ep.Timesteps 1205, Episode Reward: 289.28, Moving Average Reward: 254.35\n",
    "# Ep. 62, Ep.Timesteps 1180, Episode Reward: 289.17, Moving Average Reward: 262.54\n",
    "# Ep. 63, Ep.Timesteps 1174, Episode Reward: 292.20, Moving Average Reward: 262.86\n",
    "# Ep. 64, Ep.Timesteps 1135, Episode Reward: 293.00, Moving Average Reward: 263.07\n",
    "# Ep. 65, Ep.Timesteps 1099, Episode Reward: 294.98, Moving Average Reward: 263.38\n",
    "# Ep. 66, Ep.Timesteps 1158, Episode Reward: 292.58, Moving Average Reward: 263.59\n",
    "# Ep. 67, Ep.Timesteps 1151, Episode Reward: 293.42, Moving Average Reward: 263.81\n",
    "# Ep. 68, Ep.Timesteps 1262, Episode Reward: 286.49, Moving Average Reward: 269.16\n",
    "# Ep. 69, Ep.Timesteps 1237, Episode Reward: 288.40, Moving Average Reward: 269.37\n",
    "# Ep. 70, Ep.Timesteps 1185, Episode Reward: 291.09, Moving Average Reward: 277.29\n",
    "# ===========================\n",
    "# Problem solved in 70 episodes\n",
    "# ===========================\n",
    "# Average reward over 50 episodes:  277.28666091112905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a906daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ep. 1, Ep.Timesteps 1395, Episode Reward: 275.60\n",
    "# Ep. 2, Ep.Timesteps 1249, Episode Reward: 286.19\n",
    "# Ep. 3, Ep.Timesteps 1600, Episode Reward: -111.37\n",
    "# Ep. 4, Ep.Timesteps 1435, Episode Reward: 275.48\n",
    "# Ep. 5, Ep.Timesteps 1600, Episode Reward: 224.88\n",
    "# Ep. 6, Ep.Timesteps 1375, Episode Reward: 279.33\n",
    "# Ep. 7, Ep.Timesteps 1600, Episode Reward: 248.60\n",
    "# Ep. 8, Ep.Timesteps 1339, Episode Reward: 278.13\n",
    "# Ep. 9, Ep.Timesteps 1344, Episode Reward: 275.53\n",
    "# Ep. 10, Ep.Timesteps 1163, Episode Reward: 289.44\n",
    "# Ep. 11, Ep.Timesteps 1209, Episode Reward: 286.40\n",
    "# Ep. 12, Ep.Timesteps 1248, Episode Reward: 287.31\n",
    "# Ep. 13, Ep.Timesteps 1393, Episode Reward: 277.17\n",
    "# Ep. 14, Ep.Timesteps 1271, Episode Reward: 285.28\n",
    "# Ep. 15, Ep.Timesteps 1369, Episode Reward: 278.53\n",
    "# Ep. 16, Ep.Timesteps 1269, Episode Reward: 284.22\n",
    "# Ep. 17, Ep.Timesteps 1330, Episode Reward: 280.55\n",
    "# Ep. 18, Ep.Timesteps 1452, Episode Reward: 274.34\n",
    "# Ep. 19, Ep.Timesteps 1235, Episode Reward: 286.11\n",
    "# Ep. 20, Ep.Timesteps 1309, Episode Reward: 281.39\n",
    "# Ep. 21, Ep.Timesteps 1291, Episode Reward: 282.30\n",
    "# Ep. 22, Ep.Timesteps 1382, Episode Reward: 276.96\n",
    "# Ep. 23, Ep.Timesteps 1283, Episode Reward: 282.34\n",
    "# Ep. 24, Ep.Timesteps 1327, Episode Reward: 281.94\n",
    "# Ep. 25, Ep.Timesteps 1211, Episode Reward: 287.71\n",
    "# Ep. 26, Ep.Timesteps 1407, Episode Reward: 275.26\n",
    "# Ep. 27, Ep.Timesteps 1340, Episode Reward: 280.46\n",
    "# Ep. 28, Ep.Timesteps 1271, Episode Reward: 285.18\n",
    "# Ep. 29, Ep.Timesteps 1362, Episode Reward: 279.74\n",
    "# Ep. 30, Ep.Timesteps 1402, Episode Reward: 277.74\n",
    "# Ep. 31, Ep.Timesteps 1168, Episode Reward: 291.46\n",
    "# Ep. 32, Ep.Timesteps 1361, Episode Reward: 282.34\n",
    "# Ep. 33, Ep.Timesteps 1280, Episode Reward: 283.96\n",
    "# Ep. 34, Ep.Timesteps 1345, Episode Reward: 280.35\n",
    "# Ep. 35, Ep.Timesteps 1259, Episode Reward: 286.42\n",
    "# Ep. 36, Ep.Timesteps 1241, Episode Reward: 286.36\n",
    "# Ep. 37, Ep.Timesteps 1163, Episode Reward: 292.14\n",
    "# Ep. 38, Ep.Timesteps 1293, Episode Reward: 282.04\n",
    "# Ep. 39, Ep.Timesteps 1178, Episode Reward: 289.80\n",
    "# Ep. 40, Ep.Timesteps 1320, Episode Reward: 280.65\n",
    "# Ep. 41, Ep.Timesteps 1343, Episode Reward: 280.27\n",
    "# Ep. 42, Ep.Timesteps 1381, Episode Reward: 277.70\n",
    "# Ep. 43, Ep.Timesteps 1201, Episode Reward: 284.99\n",
    "# Ep. 44, Ep.Timesteps 1268, Episode Reward: 283.28\n",
    "# Ep. 45, Ep.Timesteps 1153, Episode Reward: 289.52\n",
    "# Ep. 46, Ep.Timesteps 1226, Episode Reward: 284.85\n",
    "# Ep. 47, Ep.Timesteps 1184, Episode Reward: 287.08\n",
    "# Ep. 48, Ep.Timesteps 1172, Episode Reward: 287.84\n",
    "# Ep. 49, Ep.Timesteps 1067, Episode Reward: 290.94\n",
    "# Ep. 50, Ep.Timesteps 1072, Episode Reward: 292.26, Moving Average Reward: 273.34\n",
    "# ===========================\n",
    "# Problem solved in 50 episodes\n",
    "# ===========================\n",
    "# Average reward over 50 episodes:  273.33972238110846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5522a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78ce67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b5a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bedd112",
   "metadata": {},
   "source": [
    "# JUST RUNNING IT AS IT FOR A VERY LONG TIME..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdda92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a5b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b9362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af88be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1788f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffc4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078ba1b5",
   "metadata": {},
   "source": [
    "Technically, we have not fully solved it until _\"getting an average reward of at least 300 over 100 consecutive trials\"_...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca631c",
   "metadata": {},
   "source": [
    "BipedalWalker-v2 defines \"solving\" as getting average reward of 300 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702af7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
